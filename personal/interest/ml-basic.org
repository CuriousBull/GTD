#+TITLE: 模型评估与选择


* 经验误差与过拟合

  1. 概念
     - 错误率：分类错误样本数占样本总数的比率
     - 精度：1-错误率
     - 误差：学习器的实际预测输出与样本的真实输出差异称为“误差”
     - 泛化误差：学习器在训练集上的误差称为“训练误差”或“经验误差”，在新样本上的误差称为“泛化误差”
     
  2. 过拟合与欠拟合
     - 过拟合：最常见的情况是由于学习能力过于强大，以至于把训练样本所包含的不太一般的特性都学到了
     - 欠拟合：通常由于学习能力低下造成

* 评估方法
  
** 留出法 
   
   #+BEGIN_QUOTE
   直接将数据集 D 划分为两个互斥的集合，其中一个集合作为训练集 S, 另一个作为测试集 T，在 S 上训练
   出模型后，用 T 来估计其测试误差，作为对泛化误差的估计。
   #+END_QUOTE
   
   - 注意点：
     1. 训练/测试集划分要尽可能保持数据分布一致性。
     2. 即便给定训练集/测试集样本比例后，仍存在多种方式对初始数据集进行划分，不同
        划分方式得到的泛化误差也会有差异。
     3. 单次使用留出法将导致评估结果不够稳定可靠，在使用留出法时，一般要曹勇若干次随机划分，重复进行
        实验评估后取平均值作为留出法的评估结果。
     4. 留出法的训练集如果比较大，会比较接近用 D 训练出的模型，但是由于 T 比较小，评估结果可能不够准确
        稳定，如果测试集 T 比较大的话，训练集 S 与 D 的差别会更大，这个缺陷没有完美解决方案，常见做法
        是将大约 $2/3\approx 4/5$ 的样本用于训练，剩余样本用于测试。

** 交叉验证法 (k 折交叉验证)

   #+BEGIN_QUOTE
   先将数据集 D 划分为 k 个大小相似的互斥子集，即 $D=D_1\cup D_2 \cup \ldots\cup D_k, D_i\cap D_j = 
   \emptyset(i\neq j)$. 每个子集 $D_i$ 都尽可能保持数据分布一致性，即从 D 中通过分层采样得到。然后，每次用
   $k-1$ 个子集的并集作为训练集，余下的那个子集作为测试集；这样，得到 k 组训练/测试集，从而可进行 k 次训练
   与测试，最终返回的是这 k 个测试结果的均值。
   #+END_QUOTE

   - 注意点：
     1. 与留出法类似，将数据集 D 划分为 k 个子集同样存在多种方式 ，为减小样本划分不同引入的差别，k 折验证法
        通常要随机使用不同的划分方式 p 次，最终这 p 次 k 折验证法结果的均值为最终的评估结果。
     2. 留一法：假定数据集 D 中包含 m 个样本，若令 k=m，则得到了交叉验证法的特例，留一法。
        - 留一法不收随机样本划分方式影响
        - 使用的训练集与初始数据集相比仅差了一个样本，使得训练结果与实际评估的模型很相似
        - 数据集比较大的情况下，计算量会很大

** 自助法

       
   #+BEGIN_QUOTE
   自助法直接以自助采样法为基础，给定 m 个样本的数据集 D，我们对其进行采样产生数据集 $D^{\prime}$ ： 每次随机从
   D 中挑选一个样本，将其拷贝放入 $D^{\prime}$, 然后再将该样本放回初始数据集 D 中，使得该样本在下次采样中仍有可
   能被采到；这个过程重复 m 次后，我们得到包含 m 个样本的数据集 $D^{\prime}$, 这就是自助采样的结果。
   #+END_QUOTE

   - 说明： 
     1. 我们希望评估的是用 D 训练出的模型，但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际
        评估的模型所使用的数据集比 D 小，这必然会引入因训练样本规模不同而导致的估计偏差，而留一法则在样本规模比较大
        的情况下，计算量太大。自助法在数据量比较小，难以有效划分训练/测试集时很有用。
 
     2. 显然，D 中有一部分数据会在 $D^{\prime}$ 中多次出现，另一部分数据则不会出现，因此，可以做一个简单估计，样本在 m
        次采样中始终不被采集到的概率是 $(1-\frac{1}{m})^m$, 取极限有：

        #+BEGIN_SRC latex :exports results
          \begin{equation}
            \lim\limits_{m\mapsto \infty}\left( 1-\frac{1}{m} \right)^m\mapsto \frac{1}{e} \approx 0.368
          \end{equation}
        #+END_SRC

        即通过自助采样，初始数据集 D 中约有 $36.8\%$ 的样本未出现在采样数据集 $D^{\prime}$ 中，于是我们可以将 $D^{\prime}$
        作为训练集，将 $D\D^{\prime}$ 作为测试集，这样，实际评估的模型与期望评估的模型都使用了 m 个训练样本，而我们
        仍有数据总量的 1/3 的、没在训练集中出现的样本用于测试，这样的测试结果，亦称 “包外估计” (=out-of-bag estimate=)

     3. 自助法能从初始数据集中产生多个不同的训练集，这对集成学习等方法有很大好处。

     4. *然而，自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差。* 因此，在初始数据量足够的情况下，留出法和交叉验证法
        更常用。

** 调参与最终模型
     
   #+BEGIN_QUOTE
   给定包含 m 个样本的数据集 D，在模型评估与选择过程中，由于需要留出一部分数据进行评估测试，事实上我们仅使用了一部分数据训练模型。
   因此，在模型选择完成后，学习算法和参数配置已经选定，此时应该用数据集 D 重新训练模型，这个模型在训练过程中使用了所有的 m 个样本，
   这才是我们最终提交给用户的模型。
   #+END_QUOTE
      
* 性能度量

  在预测任务中，给定样例集 $D=\{(\bm{x}_1,y_1),(\bm{x}_2,y_1),\ldots,(\bm{x}_m,y_m)\}$, 其中 $y_i$ 是标记示例 $\bm{x}_i$
  的真实标记。要评估学习器 $f$ 的性能，就要把学习器预测结果 $f(\bm{x})$ 与真实标记 $y$ 进行比较。

  回归任务最常用的性能度量是 “均方误差”(=mean square error=)：

  #+BEGIN_SRC latex :exports results
    \begin{equation}
      E(f;D) = \frac{1}{m}\sum\limits^{m}_{i=1}(f(\bm{x}_i)-y_i)^2
    \end{equation}
  #+END_SRC

  更一般的，对于数据分布 $\mathcal{D}$ 和概率密度函数 $p(\cdot)$, 均方误差可描述为：

  #+BEGIN_SRC latex :exports results
    \begin{equation}
      E(f;\mathcal{D}) = \int_{x\sim \mathcal{D}}(f(\bm{x}-y))^2p(\bm{x}){\rm d}\bm{x}
    \end{equation}
  #+END_SRC

** 错误率与精度

   对样例集 D, 分类错误率定义为：

   #+BEGIN_SRC latex :exports results
     \begin{equation}
       E(f;D) = \frac{1}{m}\sum\limits^{m}_{i=1}\amalg(f(\bm{x}_i)\neq{}y_i)^2
     \end{equation}
   #+END_SRC

   精度定义为：

   #+BEGIN_SRC latex :exports results
     \begin{equation}
       \begin{array}
         acc(f;D) = \frac{1}{m}\sum\limits^{m}_{i=1}\amalg(f(\bm{x}_i)={}y_i)^2\\
         =1-E(f;D)
       \end{array}
     \end{equation}
   #+END_SRC

  
* 比较检验

* 偏差与方差
